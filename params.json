{"name":"Wells-fargo-analytics-competition","tagline":"","body":"###Wells Fargo Data Challenge\r\n\r\n\r\nChris Shipp\r\nLuca Carvalho\r\nMarge Marshall\r\n\r\n\r\nApproach and Methodology\r\n\r\n\r\n    Several initial approaches were tried with the body of consumer content; the most meaningful analysis came from comparative methods.  The specific procedures we used to obtain our results are detailed herein.\r\n\r\nIn the coding process, a random sample (~10,000 posts) of the consumer content was divided into 5 categories: content mentioning BankA(1), BankB(2), BankC(3), BankD(4), or none of the four banks(5).  By using a sentiment analysis patch in the code, content was categorized as positive or negative, and each of the 5 categories was then scored according to their amount of positive vs. negative content.  This gives a rough idea of how consumers perceive each bank on a large scale.\r\n\r\nFor a deeper analysis, a semi-random sample of 80 content posts was taken and given extra classifications according to topic, cause of post, and audience. This sample was obtained by taking a random sample of 100 content posts from the first four categories, keeping with the comparative method used so far, and then determining which posts out of the 100 were meaningful.  The information gleaned from this analysis, combined with our coded analysis, provided us with a cohesive narrative regarding why people post, what they post about, and who their audience is, and how each bank is regarded comparatively.\r\n\r\n\r\n\r\n\r\n\f\r\nThe Data\r\n\r\nFor the graphs we created, each factor corresponds to one of the categories we created for the data.  For this graph and all other graphs, 0 corresponds to content regarding BankA, 1 to BankB, 2 to BankC, 3 to BankD, and 4 to no bank or other banks.\r\n\r\n\r\nAn overall sentiment score, divided by medium.  Overall, more positive content is generated on facebook.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWhen this data is split to determine scores for both negative and positive sentiment on each medium, we find that content on facebook is not only more positive, but that facebook content is more meaningful overall.\r\n\r\n\r\n\r\n\r\n\r\nOverall, BankA has the best sentiment score.  However, taking into account how content posts without strong sentiment values may affect the scores, the data is further broken down:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHere we see that even though BankA has the best overall sentiment score, the bank with the most positively valued sentiment scores (among positive sentiment posts) is BankB.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nWe can account for this difference by looking at the scores for negatively valued posts.  BankA’s negative posts have the lowest average of negative sentiment, while Bank B has a much higher negative score, decreasing its overall score.  It is notable that BankA has such little disparity.  This is discussed further in terms of the deeper content analysis.\r\n\r\nDocumented Code\r\n\r\ndf = read.table('dataset.txt',sep=\"|\",header=T)\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Grab just the texts, so you can load them in the Corpus\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\ncolnames(df.texts) = 'FullText'\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\n\r\n# Load using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(df.texts.clean))   \r\n\r\n# Strip extra whitespace\r\ndocs <- tm_map(docs, stripWhitespace)\r\n\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx]\r\n\r\nsummary(docs)\r\n\r\ndocs <- tm_map(docs, removePunctuation) \r\n\r\nbankA.dtm <- DocumentTermMatrix(bankA.docs)\r\nbankB.dtm <- DocumentTermMatrix(bankB.docs)\r\nbankC.dtm <- DocumentTermMatrix(bankC.docs)\r\nbankD.dtm <- DocumentTermMatrix(bankD.docs)\r\n\r\n# Since we can't find a great package in R, I'm going to use an\r\n# example I found online to build our own\r\n# Based on: http://www.ihub.co.ke/blogs/23216\r\n\r\n# Only need to do once\r\n# Download and upload: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\r\n#system('unrar e opinion-lexicon-English.rar')\r\n\r\npos <- scan('./AnalyticsCompetition/positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('./AnalyticsCompetition/negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  # we got a vector of sentences. plyr will handle a list\r\n  # or a vector as an \"l\" for us\r\n  # we want a simple array (\"a\") of scores back, so we use \r\n  # \"l\" + \"a\" + \"ply\" = \"laply\":\r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    # clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    # and convert to lower case:\r\n    sentence = tolower(sentence)\r\n    \r\n    # split into words. str_split is in the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    # compare our words to the dictionaries of positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    # match() returns the position of the matched term or NA\r\n    # we just want a TRUE/FALSE:\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n\r\ndf.10000$FullText = as.character(df.10000$FullText)\r\n\r\nscores = score.sentiment(df.10000$FullText, pos, neg, .progress='text')\r\nscores.500 = scores[sample(1:nrow(scores), 500), ]\r\n\r\nscores$bankA = as.numeric(grepl(\"BankA\",scores$text))\r\nscores$bankB = as.numeric(grepl(\"BankB\",scores$text))\r\nscores$bankC = as.numeric(grepl(\"BankC\",scores$text))\r\nscores$bankD = as.numeric(grepl(\"BankD\",scores$text))\r\nscores$bank = scores$bankA\r\nscores$bank[scores$bankB==1]=2\r\nscores$bank[scores$bankC==1]=3\r\nscores$bank[scores$bankD==1]=4\r\n\r\nscores.None = scores[which(scores$bank == 0), c(1,2,8,9,10)]\r\nscores.bankA = scores[which(scores$bank == 1), c(1,2,8,9,10)]\r\nscores.bankB = scores[which(scores$bank == 2), c(1,2,8,9,10)]\r\nscores.bankC = scores[which(scores$bank == 3), c(1,2,8,9,10)]\r\nscores.bankD = scores[which(scores$bank == 4), c(1,2,8,9,10)]\r\n\r\nscores$very.pos = as.numeric(scores$score >= 2)\r\nscores$very.neg = as.numeric(scores$score <= -2)\r\n\r\n# how many very positives and very negatives\r\nnumpos = sum(scores$very.pos)\r\nnumneg = sum(scores$very.neg)\r\n\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\n\r\nscores$mediatype = df.10000$MediaType\r\n\r\n#Creation of the two differwnt data frames\r\n\r\n#Marge data Frame\r\nbankA.Marge = scores.bankA[sample(1:nrow(scores.bankA), 50), ]\r\nbankA.justtweetes.M = bankA.Marge[, c(2,5)]\r\nwrite.csv(bankA.justtweetes.M, \"Tweets_bankA_Marge.csv\")\r\n\r\nbankB.Marge = scores.bankB[sample(1:nrow(scores.bankB), 50), ]\r\nbankB.justtweetes.M = bankB.Marge[, c(2,5)]\r\nwrite.csv(bankB.justtweetes.M, \"Tweets_bankB_Marge.csv\")\r\n\r\nbankC.Marge = scores.bankC[sample(1:nrow(scores.bankC), 50), ]\r\nbankC.justtweetes.M = bankC.Marge[, c(2,5)]\r\nwrite.csv(bankC.justtweetes.M, \"Tweets_bankC_Marge.csv\")\r\n\r\nbankD.Marge = scores.bankD[sample(1:nrow(scores.bankD), 50), ]\r\nbankD.justtweetes.M = bankD.Marge[, c(2,5)]\r\nwrite.csv(bankD.justtweetes.M, \"Tweets_bankD_Marge.csv\")\r\n\r\n#Chris data Frame\r\n\r\nbankA.Chris = scores.bankA[sample(1:nrow(scores.bankA), 50), ]\r\nbankA.justtweetes.C = bankA.Chris[, c(2,5)]\r\nwrite.csv(bankA.justtweetes.C, \"Tweets_bankA_Chris.csv\")\r\n\r\nbankB.Chris = scores.bankB[sample(1:nrow(scores.bankB), 50), ]\r\nbankB.justtweetes.C = bankB.Chris[, c(2,5)]\r\nwrite.csv(bankB.justtweetes.C, \"Tweets_bankB_Chris.csv\")\r\n\r\nbankC.Chris = scores.bankC[sample(1:nrow(scores.bankC), 50), ]\r\nbankC.justtweetes.C = bankC.Chris[, c(2,5)]\r\nwrite.csv(bankC.justtweetes.C, \"Tweets_bankC_Chris.csv\")\r\n\r\nbankD.Chris = scores.bankD[sample(1:nrow(scores.bankD), 50), ]\r\nbankD.justtweetes.C = bankD.Chris[, c(2,5)]\r\nwrite.csv(bankD.justtweetes.C, \"Tweets_bankD_Chris.csv\")\r\n\r\n# colors\r\ncols = c(\"#7CAE00\", \"#00BFC4\")\r\ncols2 = c(\"#FF8C00\", \"#8B0000\", \"#FF00FF\", \"#FFD700\", \"#7CAE00\")\r\nnames(cols) = c(\"twitter\", \"facebook\")\r\nnames(cols2) = c(\"0\", \"1\", \"2\", \"3\", \"4\")\r\n\r\n# boxplot\r\nlibrary(ggplot2)\r\nggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\n  geom_boxplot(aes(fill=mediatype)) +\r\n  scale_fill_manual(values=cols) +\r\n  geom_jitter(aes(color = factor(bank)),position=position_jitter(width=0.2), alpha=0.3) +\r\n  labs(title = \"Media Type's Sentiment Scores\") + \r\n  xlab('Media Type') + ylab('Sentiment Score')\r\n\r\nggplot(scores.500, aes(x=mediatype, y=score, group=mediatype)) +\r\n  geom_boxplot(aes(fill=mediatype)) +\r\n  scale_fill_manual(values=cols) +\r\n  geom_jitter(aes(color = factor(bank)),position=position_jitter(width=0.2), alpha=0.3) +\r\n  labs(title = \"Media Type's Sentiment Scores\") + \r\n  xlab('Media Type') + ylab('Sentiment Score')\r\n\r\n# barplot of average score\r\nmeanscore = tapply(scores$score, scores$mediatype, mean)\r\ndf.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\ndf.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\n\r\nggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Sentiment Score\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\n#Avarage score for each bank\r\nmeanscore.bank = tapply(scores$score, scores$bank, mean)\r\ndf.plot.bank = data.frame(bank=names(meanscore.bank), meanscore.bank=meanscore.bank)\r\ndf.plot.bank$bank <- reorder(df.plot.bank$bank, df.plot.bank$meanscore)\r\n\r\nggplot(df.plot.bank, aes(x = factor(bank), y = meanscore.bank, fill=bank)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols2[order(df.plot.bank$meanscore)]) +\r\n  labs(title = \"Average Sentiment Score For Each Bank\") + \r\n  xlab('Banks') + ylab('Average Score')\r\n\r\n# barplot of average very positive\r\nmediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\nmediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\n\r\nggplot(mediatype_pos, aes(x = factor(mediatypes), y = mediatype_pos$mean_pos, fill=mediatypes)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Positive Sentiment Score\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\n#Avarage very positive score for each bank\r\n#TODO\r\nbanks_pos = ddply(scores, .(bank), summarise, mean_pos=mean(very.pos))\r\nbanks_pos$bank <- reorder(banks_pos$bank, banks_pos$mean_pos)\r\n\r\nggplot(banks_pos, aes(x = factor(bank), y = banks_pos$mean_pos, fill=bank)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols2[order(df.plot.bank$meanscore.bank)]) +\r\n  labs(title = \"Average Very Positive Sentiment Score For Each Bank\") + \r\n  xlab('Banks') + ylab('Average Score')\r\n\r\n#Negative Words avarage\r\nmediatype_neg = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.neg))\r\nmediatype_neg$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\n\r\nggplot(mediatype_pos, aes(x = factor(mediatypes), y = mediatype_neg$mean_pos, fill=mediatypes)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Negative Sentiment Score\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\n# Negative words avarage for each bank\r\nbanks_neg = ddply(scores, .(bank), summarise, mean_pos=mean(very.neg))\r\nbanks_neg$bank <- reorder(banks_neg$bank, banks_neg$mean_pos)\r\n\r\nggplot(banks_neg, aes(x = factor(bank), y = banks_neg$mean_pos, fill=bank)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols2[order(df.plot.bank$meanscore.bank)]) +\r\n  labs(title = \"Average Very Negative Sentiment Score For Each Bank\") + \r\n  xlab('Banks') + ylab('Average Score')\r\n\r\ncreateWordCloud = function(dataset){\r\n  \r\n  corpus <- Corpus(DataframeSource(dataset))\r\n  \r\n  corpus <- tm_map(corpus, removeWords, c(\"twit_hndl\", \"ADDRESS\",\"name\", \"twit\",\"twithndlbankc\", \"hndl\", \"twit_hndl_banka\",\"twit_hndl_bankc\", \"banka\"))\r\n  corpus <- tm_map(corpus, stripWhitespace)\r\n  corpus <- tm_map(corpus, removePunctuation)\r\n  corpus <- tm_map(corpus, removeNumbers)\r\n  corpus <- tm_map(corpus, removeWords, stopwords(\"english\"))\r\n  \r\n  dtm <- DocumentTermMatrix(corpus)\r\n  tdm <- TermDocumentMatrix(corpus)\r\n  \r\n  freq <- colSums(as.matrix(dtm))   \r\n  length(freq)   \r\n  ord <- order(freq)   \r\n  m <- as.matrix(dtm)   \r\n  dim(m)   \r\n  \r\n  freq <- colSums(as.matrix(dtm))   \r\n  freq  \r\n  dtms <- removeSparseTerms(dtm, 0.25) # Prepare the data (max 15% empty space)   \r\n  freq <- colSums(as.matrix(dtm)) # Find word frequencies   \r\n  dark2 <- brewer.pal(6, \"Dark2\")   \r\n  wordcloud(names(freq), freq, max.words=150, rot.per=0.1, colors=dark2)  \r\n}\r\n\r\nlibrary(tm)\r\nlibrary(wordcloud)\r\nscores.bankC$text = as.character(scores.bankC$text)\r\nscores.bankD$text = as.character(scores.bankD$text)\r\nscores.bankB$text = as.character(scores.bankB$text)\r\nscores.bankA$text = as.character(scores.bankA$text)\r\n\r\ncreateWordCloud(scores.bankC)\r\ncreateWordCloud(scores.bankA)\r\ncreateWordCloud(scores.bankB)\r\ncreateWordCloud(scores.bankD)\r\n\r\n\r\nList of Topics and Substance\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe topics and substances we found are listed below, from highest to lowest in frequency\r\n\r\nTopics\r\nSubstances\r\ncustomer service\r\nbad experience\r\ntechnology\r\nnews\r\nPR\r\nvolunteer/gift appreciation\r\nsecurity\r\ndisapproval\r\ncard services\r\nlack of services\r\nadvertising\r\nappreciation for services\r\nunauthorized changes\r\ncustomer appreciation\r\npolicies\r\nmistrust\r\nnew customer\r\nrequest for help\r\nemployment\r\npolitics\r\nstudent services\r\naccount login\r\npersonal account\r\ncustomer attrition\r\nbank fees\r\nemployee statement\r\n\r\n\r\nidentification issue\r\n\r\n\r\ncomparing banks\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\f\r\nInsights\r\n\r\n\r\n\r\nTop Words for Bank A\r\n\r\n\r\n\r\n\r\nOverall, almost all of the content posts regarding customer service that were analyzed were caused by a bad experience. Other topics, such as PR, had greater contrast within their substance -- the majority of posts here were shared news articles, but also included some appreciation posts, mostly for Bank A and Bank B.  This agrees with our sentiment analysis that Bank A and Bank B are the banks with the highest sentiment score, and the greatest number of positive posts.  Not all of our data fits with that analysis, however;  out of the 28 posts caused by a bad experience, 10 of them were about Bank A.  Some of this is due to the nature of sentiment analysis itself, which cannot read the deeper meaning behind a sarcastic comment.  \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n“i would like to send out a special thank you to the teller at the BankA for telling everyone at the top of her lungs how much cash i just deposited into my account and the last four numbers of my account! great job and thanks for the discretion!”\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nTop Words for Bank B\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n“its been a weekend of home dedications starting with friday afternoons celebration at the Name family home with some of the amazing staff from Name island Name who helped build for salondia and her daughter! thank you tradewinds and ceo Name Name for sponsoring this home and BankB for donating the property!”\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n“has anyone else had a bad experience with BankB? their customer service(or lack there of)is terrible. maybe it is just the one teller at the lake Name branch that argued with me that i did not have an account there. i told her 3(maybe 4)times that i did. she responded each time no you dont. thanks to the lady that showed her that i did in fact have two account there. evidently the problem was that the teller arguing with me didnt realize the account was was in my name. she had my drivers license. Name!”\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nBank C and Bank D seemed to have more specific negative content.  Bank A and Bank B’s negative posts were all generally about customer services, with a few complaints about charges and fees. While Bank C also had a high number of negative customer service posts, its PR posts give us a much stronger image of how people perceive them.  Bank A and Bank B both have a mix of good and bad press, with many people expressing appreciation for the banks, and each bank totalling only 1 negative PR post each out of the sample. Out of 6 posts regarding PR for Bank C, 4 of the posts were negative.  The other 2 posts were neutral.\r\n\r\n\r\n\r\n\r\n\r\nTop Words for Bank C\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n“yes government still rolling but BankC just got their wish able to play around with risky stocks and we will bail them out again whats up with him? so disappointing”\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nBank D, whose PR posts are mostly neutral, is the only bank with any posts about security, and these posts make up the majority of Bank D’s content posts.  Of its PR posts, a considerable number of the posts are for an event, which significantly skewed the data with repeated phrases, as seen in its graph of top words.\r\n\r\nIn the deeper analysis these event related posts carried little significant meaning, despite their prevalence;  When these posts are not included, the security posts become the most notable content produced regarding Bank D.  It is clear from these posts that something has happened to cause concern for Bank D’s customers.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n “possible 83 million account are hacked at BankD Name according to another source... the figure is higher. its reported that the number is 83 million. INTERNET”\r\n\r\n\r\n\r\nTop Words for Bank D\r\n\r\n\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}